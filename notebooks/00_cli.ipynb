{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import argparse, os, sys\n",
    "import fire\n",
    "from fire import core\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from duomolog import blast\n",
    "from duomolog import msa\n",
    "from duomolog import hmmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Display(lines, out):\n",
    "    text = \"\\n\".join(lines) + \"\\n\"\n",
    "    out.write(text)\n",
    "def main():\n",
    "    commands = CLI()\n",
    "    core.Display = Display\n",
    "    fire.Fire(commands, name=\"duomolog\")\n",
    "    # print(say_hello(\"Jeremy\")==\"Hello Jeremy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duomolog command line options\n",
    "\n",
    "First attempt.\n",
    "\n",
    "This is what I have in the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test = CLI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test.blast_v_hmmer(inFile=\"data/enolase.part001.fa\",queryFile=\"data/enolase.part002.fa\" ,summaryOut=\"duomolog_out_00/summary_out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.blastOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CLI(object):\n",
    "    def __init__(self):\n",
    "        self.blastOut = None\n",
    "        self.hmmerOut = None\n",
    "    def blast_v_hmmer(self, \n",
    "\t\tinFile, \n",
    "\t\tqueryFile,\n",
    "\t\tsummaryOut=\"duomolog_out/summary_out.txt\",\n",
    "\t\tblastFile=None,\n",
    "        alnFile=None,\n",
    "\t\thmmFile=None,\n",
    "        intersectOnly=False):\n",
    "\n",
    "        os.makedirs(os.path.dirname(summaryOut), exist_ok=True)\n",
    "        Path(summaryOut).touch() # should at worst guarantee output is generated\n",
    "        outDir = '/'.join(summaryOut.split(\"/\")[:-1]) +\"/\"\n",
    "        in_nrPepDB = nrPepDB(inFile) \n",
    "        in_nrPepDB.to_csv(inFile+\".nr.csv\")\n",
    "        in_nrFile = outDir + \"in_nr.fasta\"\n",
    "        pepDB2Fa(in_nrPepDB,in_nrFile)\n",
    "\n",
    "        query_nrPepDB = nrPepDB(queryFile) \n",
    "        query_nrPepDB.to_csv(queryFile+\".nr.csv\")\n",
    "        query_novelDB, perfectMatchDB = perfectMatch(in_nrPepDB,query_nrPepDB)\n",
    "        query_novel_file = outDir + \"query_novel.fasta\"\n",
    "        perfectMatch_FileHandle = outDir + \"perfectMatch\"\n",
    "        \n",
    "        if not perfectMatchDB.empty:\n",
    "            pepDB2Fa(perfectMatchDB,f\"{perfectMatch_FileHandle}.fasta\",      \n",
    "            recordCol=\"record_y\")\n",
    "            with open(f\"{perfectMatch_FileHandle}.txt\",\"w\") as out:\n",
    "                out.write(\"queryID\\tmatchID\\n\")\n",
    "                for index, row in perfectMatchDB.iterrows():\n",
    "                    for queryID in row['allHeaders_x'].split(\";\"):\n",
    "                        for matchID in row['allHeaders_y'].split(\";\"):\n",
    "                            out.write(f\"{queryID}\\t{matchID}\\n\")\n",
    "            \n",
    "        pepDB2Fa(query_novelDB,query_novel_file)\n",
    "        \n",
    "        if blastFile == None:\n",
    "            self.blastOut = blast.run_blast(in_nrFile, query_novel_file)\n",
    "        else:\n",
    "            inHeaders = in_nrPepDB[\"1stHeader\"]\n",
    "            self.blastOut = blast.load_blast(blastFile,inHeaders)\n",
    "        \n",
    "        if hmmFile == None:\n",
    "            alnFile, alnOut = msa.run_mafft(in_nrFile)\n",
    "        self.hmmerOut = hmmer.run_hmmsearch(query_novel_file, alnFile, hmmFile)\n",
    "\n",
    "        blast_headers = set(self.blastOut[\"qseqid\"])\n",
    "        hmmer_headers = set([hit.name.decode() for hit in self.hmmerOut])\n",
    "        blast_hmmer_duo = Duo(\"blast\",blast_headers,\"hmmer\",hmmer_headers)\n",
    "        \n",
    "        self.blastOut.to_csv(f\"{outDir}blastout.txt\",sep='\\t',index=False)\n",
    "        writeOut(outDir, summaryOut, blast_hmmer_duo, query_novelDB, intersectOnly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Duo:\n",
    "\tdef __init__(self, leftName, left, rightName, right):\n",
    "\t\tself.left_union_rightName = leftName + \"_union_\" + rightName\n",
    "\t\tself.left_not_rightName = leftName + \"_not_\" + rightName\n",
    "\t\tself.left_intersect_rightName = leftName + \"_intersect_\" + rightName\n",
    "\t\tself.right_not_leftName = rightName + \"_not_\" + leftName\n",
    "\n",
    "\t\tself.subsets = {\n",
    "\t\t\tleftName: left,\n",
    "\t\t\trightName: right,\n",
    "\t\t\tself.left_union_rightName: left | right,\n",
    "\t\t\tself.left_not_rightName: left - right,\n",
    "\t\t\tself.left_intersect_rightName: left & right,\n",
    "\t\t\tself.right_not_leftName: right - left\n",
    "\t\t}\n",
    "\n",
    "\tdef dropEmpty(self):\n",
    "\t\temptySubsets = []\n",
    "\t\tfor subset in self.subsets:\n",
    "\t\t\tif self.subsets[subset] == set():\n",
    "\t\t\t\tprint(subset, \"is empty, removing it\")\n",
    "\t\t\t\temptySubsets.append(subset)\n",
    "\t\tfor emptySubset in emptySubsets: \n",
    "\t\t\tdel self.subsets[emptySubset]\n",
    "\n",
    "\tdef dropRedundant(self):\n",
    "\t\tnrSubSets = []\n",
    "\t\tredundantSubsets = []\n",
    "\t\tfor subset in self.subsets:\n",
    "\t\t\tif self.subsets[subset] not in nrSubSets:\n",
    "\t\t\t\tnrSubSets.append(self.subsets[subset])\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(subset, \"is redundant, removing it\")\n",
    "\t\t\t\tredundantSubsets.append(subset)\n",
    "\t\tfor redundantSubset in redundantSubsets:\n",
    "\t\t\tdel self.subsets[redundantSubset]\n",
    "\tdef intersectOnly(self):\n",
    "\t\tself.subsets = {self.left_intersect_rightName:self.subsets[self.left_intersect_rightName]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def writeOut(outDir,summaryOut,duo,query_novelDB, intersectOnly):\n",
    "    if intersectOnly:\n",
    "        duo.intersectOnly()\n",
    "    else:\n",
    "        duo.dropRedundant()\n",
    "    duo.dropEmpty()\n",
    "    if bool(duo):\n",
    "        with open(summaryOut, \"w\") as out:\n",
    "            for subset in duo.subsets:\n",
    "                subsetDB = subPepDB(query_novelDB, duo.subsets[subset])\n",
    "                pepDB2Fa(subsetDB, outDir + f\"{subset}.fa\")\n",
    "                for header in duo.subsets[subset]:\n",
    "                    out.write(f\"{header}\\t{subset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def nrPepDB(faFile):\n",
    "    seqDict = {}\n",
    "    outDict = {\"1stHeader\":[],  \"allHeaders\":[],\"record\":[],\"seq\":[]}\n",
    "    with open(faFile) as fa:\n",
    "        for record in SeqIO.parse(fa, \"fasta\"):\n",
    "            if record.seq not in seqDict:\n",
    "                seqDict[record.seq] = [record]\n",
    "            else:\n",
    "                print(f\"WARNING: {record.id} contains a redundant sequence\")\n",
    "                seqDict[record.seq].append(record)\n",
    "    \n",
    "    for seq in seqDict:\n",
    "        h1 = seqDict[seq][0].id\n",
    "        # allHs = ';'.join(seqDict[seq])\n",
    "        allHs = ';'.join([r.id for r in seqDict[seq]])\n",
    "        outDict[\"1stHeader\"].append(h1)\n",
    "        outDict[\"allHeaders\"].append(allHs)\n",
    "        outDict[\"record\"].append(seqDict[seq][0])\n",
    "        outDict[\"seq\"].append(str(seq))\n",
    "\n",
    "    \n",
    "    outDB = pd.DataFrame.from_dict(outDict)\n",
    "    outDB[\"1stHeader\"].astype(str)\n",
    "    return outDB\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_nrDB = nrPepDB(\"data/redundantSeqs.fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pepDB2Fa(pepDB, faFile,recordCol=\"record\",format=\"fasta\"):\n",
    "    SeqIO.write(pepDB[recordCol], faFile,format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def subPepDB(pepDB, headers):\n",
    "    return pepDB[pepDB[\"1stHeader\"].isin(headers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def perfectMatch(inDB,queryDB):\n",
    "    inSeqs = set(inDB[\"seq\"])\n",
    "    querySeqs = set(queryDB[\"seq\"])\n",
    "    # print(inSeqs)\n",
    "    # print(querySeqs)\n",
    "    sharedSeqs = inSeqs & querySeqs\n",
    "    sharedDB = queryDB[queryDB[\"seq\"].isin(sharedSeqs)]\n",
    "    inner_mergeDB = inner_mergePD(inDB,sharedDB,\"seq\")\n",
    "    novelDB = queryDB[~queryDB[\"seq\"].isin(sharedSeqs)]\n",
    "    \n",
    "\n",
    "\n",
    "    return novelDB, inner_mergeDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def inner_mergePD(x,y,commonCol):\n",
    "    return x.merge(y, on = commonCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def getOtherColumnValue(df,c1,c2,c1_value):\n",
    "    # return df[df[c1] == c1_value][c2][0]\n",
    "    return df.loc[df[c1] == c1_value, c2].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "getOtherColumnValue(p1_nrDB,\"1stHeader\",\"seq\",\"4416387\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "p1_nrDB = nrPepDB(\"data/enolase.part001.fa\")\n",
    "p2_nrDB = nrPepDB(\"data/enolase.part002.fa\")\n",
    "\n",
    "p2_novelDB, p2_shared_with_p1 = perfectMatch(p1_nrDB,p2_nrDB)\n",
    "\n",
    "p2_shared_with_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "not p2_shared_with_p1.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "for index, row in p2_insert_seqFrom_p1_shared.iterrows():\n",
    "    for queryID in row['allHeaders_x'].split(\";\"):\n",
    "        for matchID in row['allHeaders_y'].split(\";\"):\n",
    "            print(queryID,matchID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_insert_seqFrom_p1DB = nrPepDB(\"data/enolase.part002.insert_seqFrom_part1.fa\")\n",
    "p2_insert_seqFrom_p1_novel , p2_insert_seqFrom_p1_shared = perfectMatch(p1_nrDB,p2_insert_seqFrom_p1DB)\n",
    "p2_insert_seqFrom_p1_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_insert_seqFrom_p1_shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we see that 17137654_copy is shared with p1_nrDB, we need to get the header that it matched with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_mergePD(p1_nrDB,p2_insert_seqFrom_p1_shared,\"seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_nrDB.merge(p2_insert_seqFrom_p1_shared, on = \"seq\")\n",
    "# \"17137654_copy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "p2_novelDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_inDB = pd.DataFrame.from_dict({\"seq\":[1,2,3],\"h\":[1,2,3]})\n",
    "test_queryDB = pd.DataFrame.from_dict({\"seq\":[2,4,6],\"h\":[1,2,3]})\n",
    "perfectMatch(test_inDB,test_queryDB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duomolog",
   "language": "python",
   "name": "duomolog"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
